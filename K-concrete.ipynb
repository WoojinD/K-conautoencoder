{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout, LeakyReLU, Softmax, Layer\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.optimizers import Optimizer\n",
    "from keras.legacy import interfaces\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import Constant, glorot_normal\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import metrics\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteSelect(Layer):\n",
    "    def __init__(self, output_dim, start_temp = 10.0, min_temp = 0.1, alpha = 0.99999, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.start_temp = start_temp\n",
    "        self.min_temp = K.constant(min_temp)\n",
    "        self.alpha = K.constant(alpha)\n",
    "        super(ConcreteSelect, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.temp = self.add_weight(name = 'temp', shape = [], initializer = Constant(self.start_temp), trainable = False)\n",
    "        self.logits = self.add_weight(name = 'logits', shape = [self.output_dim, input_shape[1]], initializer = glorot_normal(), trainable = True)\n",
    "        super(ConcreteSelect, self).build(input_shape)\n",
    "        \n",
    "    def call(self, X, training = None):\n",
    "        uniform = K.random_uniform(self.logits.shape, K.epsilon(), 1.0)\n",
    "        gumbel = -K.log(-K.log(uniform))\n",
    "        temp = K.update(self.temp, K.maximum(self.min_temp, self.temp * self.alpha))\n",
    "        noisy_logits = (self.logits + gumbel) / temp\n",
    "        samples = K.softmax(noisy_logits)\n",
    "        discrete_logits = K.one_hot(K.argmax(self.logits), self.logits.shape[1])\n",
    "\n",
    "        self.selections = K.in_train_phase(samples, discrete_logits, training)\n",
    "        Y = K.dot(X, K.transpose(self.selections))\n",
    "        return Y\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "    \n",
    "class StopperCallback(EarlyStopping):\n",
    "    \n",
    "    def __init__(self, mean_max_target = 0.998):\n",
    "        self.mean_max_target = mean_max_target\n",
    "        super(StopperCallback, self).__init__(monitor = '', patience = float('inf'), verbose = 1, mode = 'max', baseline = self.mean_max_target)\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        print('mean max of probabilities:', self.get_monitor_value(logs), '- temperature', K.get_value(self.model.get_layer('concrete_select').temp))\n",
    "        #print( K.get_value(K.max(K.softmax(self.model.get_layer('concrete_select').logits), axis = -1)))\n",
    "        #print(K.get_value(K.max(self.model.get_layer('concrete_select').selections, axis = -1)))\n",
    "    \n",
    "    def get_monitor_value(self, logs):\n",
    "        monitor_value = K.get_value(K.mean(K.max(K.softmax(self.model.get_layer('concrete_select').logits), axis = -1)))\n",
    "        return monitor_value\n",
    "\n",
    "\n",
    "class ConcreteAutoencoderFeatureSelector():\n",
    "    def __init__(self, K, output_function, num_epochs = 300, batch_size = None, learning_rate = 0.001, start_temp = 10.0, min_temp = 0.1, tryout_limit = 5):\n",
    "\n",
    "        self.K = K\n",
    "        self.output_function = output_function\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.start_temp = start_temp\n",
    "        self.min_temp = min_temp\n",
    "        self.tryout_limit = tryout_limit\n",
    "        \n",
    "    def fit(self, X, Y, val_X = None, val_Y = None):\n",
    "        assert len(X) == len(Y)\n",
    "        validation_data = None\n",
    "        if val_X is not None and val_Y is not None:\n",
    "            assert len(val_X) == len(val_Y)\n",
    "            validation_data = (val_X, val_Y)\n",
    "        \n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = max(len(X) // 256, 16)\n",
    "        \n",
    "        num_epochs = self.num_epochs\n",
    "        steps_per_epoch = (len(X) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        for i in range(self.tryout_limit):\n",
    "\n",
    "            K.set_learning_phase(1)\n",
    "\n",
    "            inputs = Input(shape = X.shape[1:])\n",
    "\n",
    "            alpha = math.exp(math.log(self.min_temp / self.start_temp) / (num_epochs * steps_per_epoch))\n",
    "            \n",
    "            self.concrete_select = ConcreteSelect(self.K, self.start_temp, self.min_temp, alpha, name = 'concrete_select')\n",
    "\n",
    "            selected_features = self.concrete_select(inputs)\n",
    "\n",
    "            outputs = self.output_function(selected_features)\n",
    "\n",
    "            self.model = Model(inputs, outputs)\n",
    "\n",
    "            self.model.compile(Adam(self.learning_rate), loss = 'mean_squared_error')\n",
    "            \n",
    "            print(self.model.summary())\n",
    "            \n",
    "            stopper_callback = StopperCallback()\n",
    "            \n",
    "            hist = self.model.fit(X, Y, self.batch_size, num_epochs, verbose = 1, callbacks = [stopper_callback], validation_data = validation_data)#, validation_freq = 10)\n",
    "            \n",
    "            if K.get_value(K.mean(K.max(K.softmax(self.concrete_select.logits, axis = -1)))) >= stopper_callback.mean_max_target:\n",
    "                break\n",
    "            \n",
    "            num_epochs *= 2\n",
    "        \n",
    "        self.probabilities = K.get_value(K.softmax(self.model.get_layer('concrete_select').logits))\n",
    "        self.indices = K.get_value(K.argmax(self.model.get_layer('concrete_select').logits))\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def get_indices(self):\n",
    "        return K.get_value(K.argmax(self.model.get_layer('concrete_select').logits))\n",
    "    \n",
    "    def get_mask(self):\n",
    "        return K.get_value(K.sum(K.one_hot(K.argmax(self.model.get_layer('concrete_select').logits), self.model.get_layer('concrete_select').logits.shape[1]), axis = 0))\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.get_indices()]\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def get_support(self, indices = False):\n",
    "        return self.get_indices() if indices else self.get_mask()\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(30, 40)\n",
      "(30,)\n",
      "[[-4.6031   -4.7731   -0.55654  ...  1.0816   -0.4595    1.6364  ]\n",
      " [-4.5303   -4.8055   -2.5669   ... -2.164    -1.8616   -1.3002  ]\n",
      " [-5.3179   -4.231    -1.2701   ... -0.19774   3.0119   -1.3725  ]\n",
      " ...\n",
      " [ 4.7557    5.5796    0.26973  ...  0.70312   0.49155   0.91268 ]\n",
      " [ 5.1628    6.0414   -3.1397   ... -0.4027   -0.14583  -1.7414  ]\n",
      " [ 4.8887    4.5366   -2.7128   ... -0.064862  0.93427  -1.2405  ]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "# normalize all values between 0 and 1 and flatten the 28x28 images into vectors of size 784\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "leuk_data=np.loadtxt('simulationdata30_2.csv',delimiter=',')\n",
    "print(leuk_data.shape)\n",
    "x_train=leuk_data;\n",
    "\n",
    "leuk_label=np.loadtxt('simulationdata30_label.csv',delimiter=',')\n",
    "print(leuk_label.shape)\n",
    "y_train=leuk_label-1;\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_with_kmeans(dims, act='relu', init='glorot_uniform', K=5, num_epochs = 300, batch_size = None, learning_rate = 0.001, start_temp = 10.0, min_temp = 0.1, tryout_limit = 5):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "\n",
    "    # input\n",
    "    clustering_input = Input(shape=(dims[-1],), name='clustering_input')\n",
    "    \n",
    "    if batch_size is None:\n",
    "        batch_size = 10 # max(dims[0] // 256, 16)\n",
    "    \n",
    "    steps_per_epoch = (dims[0] + batch_size - 1) // batch_size\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    alpha = math.exp(math.log(min_temp / start_temp) / (num_epochs * steps_per_epoch))\n",
    "    concrete_select = ConcreteSelect(K , start_temp, min_temp, alpha, name = 'concrete_select')\n",
    "    selected_features = concrete_select(x)\n",
    "    h = (selected_features)\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks - 1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(\n",
    "        h)  # hidden layer, features are extracted from here\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks - 1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "        y = LeakyReLU(0.2)(y)\n",
    "        y = Dropout(0.1)(y)\n",
    "        \n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=[x, clustering_input], outputs=y, name='AE'), Model(inputs=x, outputs=h,\n",
    "                                                                            name='encoder'), clustering_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_wrapper(encoded_X, label_centers, lambd):\n",
    "    def loss(y_true, y_pred):\n",
    "        cost_clustering = K.mean(K.square(label_centers - encoded_X), axis=-1)\n",
    "        cost_reconstruction = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "        cost = lambd * cost_clustering + cost_reconstruction\n",
    "        return cost\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class DCN(object):\n",
    "    def __init__(self, dims, n_clusters, lambd=0.5, init='glorot_uniform'):\n",
    "        super(DCN, self).__init__()\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.lambd = lambd\n",
    "        self.autoencoder, self.encoder, self.clustering_input = autoencoder_with_kmeans(self.dims, init=init)\n",
    "\n",
    "        self.centers = np.zeros((self.n_clusters, self.dims[-1]))\n",
    "        self.count = 100 * np.ones(self.n_clusters, dtype=np.int)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=10, save_dir='results/temp'):\n",
    "        \n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if int(epochs / 10) != 0 and epoch % int(epochs / 10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    print(' ' * 8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (metrics.acc(self.y, y_pred), metrics.nmi(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time()\n",
    "        temp = []\n",
    "        for i in range(x.shape[0]):\n",
    "            t = []\n",
    "            for k in range(self.dims[-1]):\n",
    "                t.append(0)\n",
    "            temp.append(t)\n",
    "        temp = np.array(temp)\n",
    "        self.autoencoder.fit([x, temp], x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: %ds' % round(time() - t0))\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights_200.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights_200.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def init_centers(self, x, y=None):\n",
    "        # init self.centers\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters)\n",
    "        kmeans.fit(self.encoder.predict(x))\n",
    "        self.all_pred = kmeans.labels_\n",
    "        self.centers = kmeans.cluster_centers_\n",
    "\n",
    "        print('centers-', self.centers)\n",
    "        if y is not None:\n",
    "            self.metric(y, self.all_pred)\n",
    "\n",
    "    def compile(self):\n",
    "        self.autoencoder.compile(optimizer='adam', loss=loss_wrapper(self.encoder.output, self.clustering_input, 0.5))\n",
    "\n",
    "    def fit(self, x, y, epoches, batch_size=10, save_dir='./models'):\n",
    "        m = x_train.shape[0]\n",
    "        self.count = 100 * np.ones(self.n_clusters, dtype=np.int)\n",
    "        for step in range(epoches):\n",
    "            cost = []  # all cost\n",
    "\n",
    "            for batch_index in range(int(m / batch_size) + 1):\n",
    "                X_batch = x[batch_index * batch_size:(batch_index + 1) * batch_size, :]\n",
    "\n",
    "                labels_of_centers = self.centers[self.all_pred[batch_index * batch_size:(batch_index + 1) * batch_size]]\n",
    "\n",
    "                c1 = self.autoencoder.train_on_batch([X_batch, labels_of_centers], X_batch)\n",
    "                cost.append(c1)\n",
    "\n",
    "                reductX = self.encoder.predict(X_batch)\n",
    "\n",
    "                # update k-means\n",
    "                self.all_pred[\n",
    "                batch_index * batch_size:(batch_index + 1) * batch_size], self.centers, self.count = self.batch_km(\n",
    "                    reductX, self.centers, self.count)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                reductX = self.encoder.predict(x)\n",
    "                km_model = KMeans(self.n_clusters, init=self.centers)\n",
    "                self.all_pred = km_model.fit_predict(reductX)\n",
    "                self.centers = km_model.cluster_centers_\n",
    "\n",
    "                print('step-', step, ' cost:', np.mean(cost))\n",
    "                #                print('centers-',self.centers)\n",
    "                print('count-', self.count)\n",
    "                self.metric(y, self.all_pred)\n",
    "        print('saving model to:', save_dir + '/DCN_model_final.h5')\n",
    "        self.autoencoder.save_weights(save_dir + '/DCN_model_final.h5')\n",
    "\n",
    "    def batch_km(self, data, center, count):\n",
    "        \"\"\"\n",
    "        Function to perform a KMeans update on a batch of data, center is the\n",
    "        centroid from last iteration.\n",
    "\n",
    "        \"\"\"\n",
    "        N = data.shape[0]\n",
    "        K = center.shape[0]\n",
    "\n",
    "        # update assignment\n",
    "        idx = np.zeros(N, dtype=np.int)\n",
    "        for i in range(N):\n",
    "            dist = np.inf\n",
    "            ind = 0\n",
    "            for j in range(K):\n",
    "                temp_dist = np.linalg.norm(data[i] - center[j])\n",
    "                if temp_dist < dist:\n",
    "                    dist = temp_dist\n",
    "                    ind = j\n",
    "            idx[i] = ind\n",
    "\n",
    "        # update centriod\n",
    "        center_new = center\n",
    "        for i in range(N):\n",
    "            c = idx[i]\n",
    "            count[c] += 1\n",
    "            eta = 1.0 / count[c]\n",
    "            center_new[c] = (1 - eta) * center_new[c] + eta * data[i]\n",
    "        center_new.astype(np.float32)\n",
    "        return idx, center_new, count\n",
    "\n",
    "    def get_centers_and_types_of_points(self, reductX):\n",
    "        distances = np.abs(reductX - self.centers[:, np.newaxis])\n",
    "        label_types = np.min(np.argmin(distances, axis=0), axis=1)\n",
    "        labels_of_centers = self.centers[label_types]\n",
    "        return labels_of_centers, label_types\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.autoencoder.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        reductX = self.encoder.predict(x)\n",
    "        labels_of_centers, label_types = self.get_centers_and_types_of_points(reductX)\n",
    "        return label_types\n",
    "\n",
    "    def metric(self, y, y_pred):\n",
    "        acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "        nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "        ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "        print(y_pred)\n",
    "        print('acc:', acc)\n",
    "        print('nmi:', nmi)\n",
    "        print('ari:', ari)\n",
    "\n",
    "    def get_indices(self):\n",
    "        print('indices')\n",
    "        print(K.get_value(K.argmax(self.encoder.get_layer('concrete_select').logits)))\n",
    "        return K.get_value(K.argmax(self.encoder.get_layer('concrete_select').logits))     \n",
    "    \n",
    "    def get_support(self, indices = False):\n",
    "        return self.get_indices() if indices else self.get_mask()\n",
    "    \n",
    "    def get_mask(self):\n",
    "        print('indices2')\n",
    "        print(K.get_value(K.sum(K.one_hot(K.argmax(self.encoder.get_layer('concrete_select').logits), self.model.get_layer('concrete_select').logits.shape[1]), axis = 0)))\n",
    "        return K.get_value(K.sum(K.one_hot(K.argmax(self.encoder.get_layer('concrete_select').logits), self.model.get_layer('concrete_select').logits.shape[1]), axis = 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(10, 40), b.shape=(40, 5), m=10, n=5, k=40\n\t [[Node: concrete_select/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/concrete_select/MatMul_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_input_0_0/_167, concrete_select/transpose)]]\n\t [[Node: loss_1/mul/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1263_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b18ceee7ba38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mae_weights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpretrain_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdcn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpretrain_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdcn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mae_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-0d50291cff9d>\u001b[0m in \u001b[0;36mpretrain\u001b[1;34m(self, x, y, optimizer, epochs, batch_size, save_dir)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Pretraining time: %ds'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/ae_weights_200.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythonProject10\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythonProject10\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythonProject10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythonProject10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythonProject10\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythonProject10\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(10, 40), b.shape=(40, 5), m=10, n=5, k=40\n\t [[Node: concrete_select/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/concrete_select/MatMul_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_input_0_0/_167, concrete_select/transpose)]]\n\t [[Node: loss_1/mul/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1263_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "dcn = DCN(dims=[x_train.shape[-1], 500, 500, 2000, 10], n_clusters=4, lambd=0.001)\n",
    "dcn.compile()\n",
    "\n",
    "\n",
    "#ae_weights = 'results/temp/ae_weights_200.h5'\n",
    "ae_weights=None\n",
    "\n",
    "if ae_weights is None:\n",
    "    pretrain_epochs = 2000\n",
    "    dcn.pretrain(x=x_train, epochs=pretrain_epochs)\n",
    "else:\n",
    "    dcn.autoencoder.load_weights(ae_weights)\n",
    "dcn.init_centers(x_train, y_train)\n",
    "\n",
    "dcn.fit(x_train, y_train, epoches=3200, batch_size=20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = dcn.predict(x_test)\n",
    "#print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dcn.metric(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn.get_support(indices = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject10",
   "language": "python",
   "name": "pythonproject10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
